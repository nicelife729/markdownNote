一个 Elasticsearch 集群可以 包含多个 索引 ，相应的每个索引可以包含多个 类型 。 这些不同的类型存储着多个 文档 ，每个文档又有 多个 属性 。

索引实际上是指向一个或者多个物理 分片 的 逻辑命名空间 。

一个 分片 是一个底层的 工作单元 ，它仅保存了 全部数据中的一部分。 在分片内部机制中，我们将详细介绍分片是如何工作的，而现在我们只需知道一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。

Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。

一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。

一个副本分片只是一个主分片的拷贝。 副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。

在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。


#文档

通常情况下，我们使用的术语 对象 和 文档 是可以互相替换的。

##文档元数据
_index
文档在哪存放
_type
文档表示的对象类别
_id
文档唯一标识

一个 索引 应该是因共同的特性被分组到一起的文档集合。
这个名字必须小写，不能以下划线开头，不能包含逗号。

一个 _type 命名可以是大写或者小写，但是不能以下划线或者句号开头，不应该包含逗号， 并且长度限制为256个字符

ID 是一个字符串， 当它和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档。


使用自定义的 ID
如果你的文档有一个自然的 标识符


在 Elasticsearch 中文档是 不可改变 的，不能修改它们。

##取回多个文档
lasticsearch 的速度已经很快了，但甚至能更快。 将多个请求合并成一个，避免单独处理每个请求花费的网络时延和开销。 如果你需要从 Elasticsearch 检索很多文档，那么使用 multi-get 或者 mget API 来将这些检索请求放在一个请求中，将比逐个文档请求更快地检索到全部文档。

mget API 要求有一个 docs 数组作为参数，每个 元素包含需要检索文档的元数据， 包括 _index 、 _type 和 _id 。

##bulk
bulk 请求不是原子的： 不能用它来实现事务控制。每个请求是单独处理的，因此一个请求的成功或失败不会影响其他的请求。


所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。我们也会在扩容设计这一章中详细讨论为什么会有这样一种需求。





为了充分挖掘 Elasticsearch 的潜力，你需要理解以下三个概念：

映射（Mapping）
描述数据在每个字段内如何存储
分析（Analysis）
全文是如何处理使之可以被搜索的
领域特定查询语言（Query DSL）
Elasticsearch 中强大灵活的查询语言



#分页
和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数：

size
显示应该返回的结果数量，默认是 10
from
显示应该跳过的初始结果数量，默认是 0



Elasticsearch 中的数据可以概括的分为两类：精确值和全文。


精确值 如它们听起来那样精确。例如日期或者用户 ID，但字符串也可以表示精确值，例如用户名或邮箱地址。对于精确值来讲，Foo 和 foo 是不同的，2014 和 2014-09-15 也是不同的。

另一方面，全文 是指文本数据（通常以人类容易识别的语言书写），例如一个推文的内容或一封邮件的内容


查询全文数据要微妙的多。我们问的不只是“这个文档匹配查询吗”，而是“该文档匹配查询的程度有多大？”换句话说，该文档与给定查询的相关性如何？


Elasticsearch 首先 分析 文档，之后根据结果创建 倒排索引 。在接下来的两节，我们会讨论倒排索引和分析过程。


#倒排索引
Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。

分词和标准化的过程称为 分析 ， 我们会在下个章节讨论。


全文查询，理解每个域是如何定义的，因此它们可以做 正确的事：

当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。
当你查询一个 精确值 域时，不会分析查询字符串， 而是搜索你指定的精确值。


#映射
##自定义域映射
尽管在很多情况下基本域数据类型 已经够用，但你经常需要为单独域自定义映射 ，特别是字符串域。自定义映射允许你执行下面的操作：

全文字符串域和精确值字符串域的区别
使用特定语言分析器
优化域以适应部分匹配
指定自定义数据格式
还有更多
域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ：

###index
index 属性控制怎样索引字符串。它可以是下面三个值：

analyzed
首先分析字符串，然后索引它。换句话说，以全文索引这个域。
not_analyzed
  索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。
no
不索引这个域。这个域不会被搜索到。


##复杂核心域类型
除了我们提到的简单标量数据类型， JSON 还有 null 值，数组，和对象，这些 Elasticsearch 都是支持的。

！！每个多值域只是一包无序的值，而不是有序数组



一个查询语句 的典型结构：

{
    QUERY_NAME: {
        ARGUMENT: VALUE,
        ARGUMENT: VALUE,...
    }
}
如果是针对某个字段，那么它的结构如下：

{
    QUERY_NAME: {
        FIELD_NAME: {
            ARGUMENT: VALUE,
            ARGUMENT: VALUE,...
        }
    }
}


##合并查询语句
查询语句(Query clauses) 就像一些简单的组合块 ，这些组合块可以彼此之间合并组成更复杂的查询。这些语句可以是如下形式：

叶子语句（Leaf clauses） (就像 match 语句) 被用于将查询字符串和一个字段（或者多个字段）对比。
复合(Compound) 语句 主要用于 合并其它查询语句。 比如，一个 bool 语句 允许在你需要的时候组合其它语句，无论是 must 匹配、 must_not 匹配还是 should 匹配，同时它可以包含不评分的过滤器（filters）：



#查询与过滤
 Elasticsearch 使用的查询语言（DSL） 拥有一套查询组件，这些组件可以以无限组合的方式进行搭配。这套组件可以在以下两种情况下使用：过滤情况（filtering context）和查询情况（query context）。
 
 
 当使用于 过滤情况 时，查询被设置成一个“不评分”或者“过滤”查询。即，这个查询只是简单的问一个问题：“这篇文档是否匹配？”。回答也是非常的简单，yes 或者 no ，二者必居其一。
 
 当使用于 查询情况 时，查询就变成了一个“评分”的查询。和不评分的查询类似，也要去判断这个文档是否匹配，同时它还需要判断这个文档匹配的有 _多好_（匹配程度如何）。
 
 ###性能差异
 过滤查询（Filtering queries）只是简单的检查包含或者排除，这就使得计算起来非常快。考虑到至少有一个过滤查询（filtering query）的结果是 “稀少的”（很少匹配的文档），并且经常使用不评分查询（non-scoring queries），结果会被缓存到内存中以便快速读取，所以有各种各样的手段来优化查询结果。
 
 相反，评分查询（scoring queries）不仅仅要找出 匹配的文档，还要计算每个匹配文档的相关性，计算相关性使得它们比不评分查询费力的多。同时，查询结果并不缓存。
 
 多亏倒排索引（inverted index），一个简单的评分查询在匹配少量文档时可能与一个涵盖百万文档的filter表现的一样好，甚至会更好。但是在一般情况下，一个filter 会比一个评分的query性能更优异，并且每次都表现的很稳定。
 
 过滤（filtering）的目标是减少那些需要通过评分查询（scoring queries）进行检查的文档。
 
 
 
 #重要的查询
 ##range 查询
   range 查询找出那些落在指定区间内的数字或者时间：
   被允许的操作符如下：
   
   gt
   大于
   gte
   大于等于
   lt
   小于
   lte
   小于等于
   
 ##term 查询
   term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串：
   
   { "term": { "age":    26           }}
   { "term": { "date":   "2014-09-01" }}
   { "term": { "public": true         }}
   { "term": { "tag":    "full_text"  }}
   
   term 查询对于输入的文本不 分析 ，所以它将给定的值进行精确查询。
   
 ##terms 查询
   terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件：
   
   { "terms": { "tag": [ "search", "full_text", "nosql" ] }}
   
   和 term 查询一样，terms 查询对于输入的文本不分析。它查询那些精确匹配的值（包括在大小写、重音、空格等方面的差异）。
   
 ##exists 查询和 missing 查询
   exists 查询和 missing 查询被用于查找那些指定字段中有值 (exists) 或无值 (missing) 的文档。这与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性：
   
   {
       "exists":   {
           "field":    "title"
       }
   }
   
   这些查询经常用于某个字段有值的情况和某个字段缺值的情况
   
  #组合多查询
   现实的查询需求从来都没有那么简单；它们需要在多个字段上查询多种多样的文本，并且根据一系列的标准来过滤。为了构建类似的高级查询，你需要一种能够将多查询组合成单一查询的查询方法。
   
   你可以用 bool 查询来实现你的需求。这种查询将多查询组合在一起，成为用户自己想要的布尔查询。它接收以下参数：
   
   must
   文档 必须 匹配这些条件才能被包含进来。
   must_not
   文档 必须不 匹配这些条件才能被包含进来。
   should
   如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。
   filter
   必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。
   
   
   ##增加带过滤器（filtering）的查询
   如果我们不想因为文档的时间而影响得分，可以用 filter 语句来重写前面的例子：
   
   {
       "bool": {
           "must":     { "match": { "title": "how to make millions" }},
           "must_not": { "match": { "tag":   "spam" }},
           "should": [
               { "match": { "tag": "starred" }}
           ],
           "filter": {
             "range": { "date": { "gte": "2014-01-01" }} 
           }
       }
   }
   
   
   range 查询已经从 should 语句中移到 filter 语句
   
   通过将 range 查询移到 filter 语句中，我们将它转成不评分的查询，将不再影响文档的相关性排名。由于它现在是一个不评分的查询，可以使用各种对 filter 查询有效的优化手段来提升性能。
   
   所有查询都可以借鉴这种方式。将查询移到 bool 查询的 filter 语句中，这样它就自动的转成一个不评分的 filter 了。
   
   
   #排序
   ##按照字段的值排序
   在这个案例中，通过时间来对 tweets 进行排序是有意义的，最新的 tweets 排在最前。 我们可以使用 sort 参数进行实现：
   
   GET /_search
   {
       "query" : {
           "bool" : {
               "filter" : { "term" : { "user_id" : 1 }}
           }
       },
       "sort": { "date": { "order": "desc" }}
   }
   拷贝为 CURL在 SENSE 中查看 
   你会注意到结果中的两个不同点：
   
   "hits" : {
       "total" :           6,
       "max_score" :       null, 
       "hits" : [ {
           "_index" :      "us",
           "_type" :       "tweet",
           "_id" :         "14",
           "_score" :      null, 
           "_source" :     {
                "date":    "2014-09-24",
                ...
           },
           "sort" :        [ 1411516800000 ] 
       },
       ...
   }
    
   
   _score 不被计算, 因为它并没有用于排序。
   
   
   
   date 字段的值表示为自 epoch (January 1, 1970 00:00:00 UTC)以来的毫秒数，通过 sort 字段的值进行返回。
   
   首先我们在每个结果中有一个新的名为 sort 的元素，它包含了我们用于排序的值。 在这个案例中，我们按照 date 进行排序，在内部被索引为 自 epoch 以来的毫秒数 。 long 类型数 1411516800000 等价于日期字符串 2014-09-24 00:00:00 UTC 。
   
   其次 _score 和 max_score 字段都是 null 。 计算 _score 的花销巨大，通常仅用于排序； 我们并不根据相关性排序，所以记录 _score 是没有意义的。如果无论如何你都要计算 _score ， 你可以将 track_scores 参数设置为 true 。
   
   提示
   一个简便方法是, 你可以 指定一个字段用来排序：
   
       "sort": "number_of_children"
   字段将会默认升序排序 ，而按照 _score 的值进行降序排序。
   

假定我们想要结合使用 date 和 _score 进行查询，并且匹配的结果首先按照日期排序，然后按照相关性排序：

##多级排序
GET /_search
{
    "query" : {
        "bool" : {
            "must":   { "match": { "tweet": "manage text search" }},
            "filter" : { "term" : { "user_id" : 2 }}
        }
    },
    "sort": [
        { "date":   { "order": "desc" }},
        { "_score": { "order": "desc" }}
    ]
}
排序条件的顺序是很重要的。结果首先按第一个条件排序，仅当结果集的第一个 sort 值完全相同时才会按照第二个条件进行排序，以此类推。

多级排序并不一定包含 _score 。你可以根据一些不同的字段进行排序， 如地理距离或是脚本计算的特定值。

##字段多值的排序
一种情形是字段有多个值的排序， 需要记住这些值并没有固有的顺序；一个多值的字段仅仅是多个值的包装，这时应该选择哪个进行排序呢？

对于数字或日期，你可以将多值字段减为单值，这可以通过使用 min 、 max 、 avg 或是 sum 排序模式 。 例如你可以按照每个 date 字段中的最早日期进行排序，通过以下方法：

"sort": {
    "dates": {
        "order": "asc",
        "mode":  "min"
    }
}

#Doc Values 介绍
 本章的最后一个话题是关于 Elasticsearch 内部的一些运行情况。在这里我们先不介绍新的知识点，所以我们应该意识到，Doc Values 是我们需要反复提到的一个重要话题。
 
 当你对一个字段进行排序时，Elasticsearch 需要访问每个匹配到的文档得到相关的值。倒排索引的检索性能是非常快的，但是在字段值排序时却不是理想的结构。
 
 在搜索的时候，我们能通过搜索关键词快速得到结果集。
 当排序的时候，我们需要倒排索引里面某个字段值的集合。换句话说，我们需要 ``倒置`` 倒排索引。
 ``倒置`` 结构在其他系统中经常被称作 ``列存储`` 。实质上，它将所有单字段的值存储在单数据列中，这使得对其进行操作是十分高效的，例如排序。
 
 在 Elasticsearch 中，doc values 就是一种列式存储结构，默认情况下每个字段的 doc values 都是激活的，doc values 是在索引时创建的，当字段索引时，Elasticsearch 为了能够快速检索，会把字段的值加入倒排索引中，同时它也会存储该字段的 doc values。
 
 Elasticsearch 中的 doc vaules 常被应用到以下场景：
 
 对一个字段进行排序
 对一个字段进行聚合
 某些过滤，比如地理位置过滤
 某些与字段相关的脚本计算
 因为文档值被序列化到磁盘，我们可以依靠操作系统的帮助来快速访问。当 working set 远小于节点的可用内存，系统会自动将所有的文档值保存在内存中，使得其读写十分高速； 当其远大于可用内存，操作系统会自动把 doc values 加载到系统的页缓存中，从而避免了 jvm 堆内存溢出异常。
 
 我们稍后会深入讨论 doc values。现在所有你需要知道的是排序发生在索引时建立的平行数据结构中。
 
 
 #Doc Values
 聚合使用一个叫 doc values 的数据结构（在 Doc Values 介绍 里简单介绍）。 Doc values 可以使聚合更快、更高效并且内存友好，所以理解它的工作方式十分有益。
 
 Doc values 的存在是因为倒排索引只对某些操作是高效的。 倒排索引的优势 在于查找包含某个项的文档，而对于从另外一个方向的相反操作并不高效，即：确定哪些项是否存在单个文档里，聚合需要这种次级的访问模式。
 
 对于以下倒排索引：
 
 Term      Doc_1   Doc_2   Doc_3
 ------------------------------------
 brown   |   X   |   X   |
 dog     |   X   |       |   X
 dogs    |       |   X   |   X
 fox     |   X   |       |   X
 foxes   |       |   X   |
 in      |       |   X   |
 jumped  |   X   |       |   X
 lazy    |   X   |   X   |
 leap    |       |   X   |
 over    |   X   |   X   |   X
 quick   |   X   |   X   |   X
 summer  |       |   X   |
 the     |   X   |       |   X
 ------------------------------------
 如果我们想要获得所有包含 brown 的文档的词的完整列表，我们会创建如下查询：
 
 GET /my_index/_search
 {
   "query" : {
     "match" : {
       "body" : "brown"
     }
   },
   "aggs" : {
     "popular_terms": {
       "terms" : {
         "field" : "body"
       }
     }
   }
 }
 查询部分简单又高效。倒排索引是根据项来排序的，所以我们首先在词项列表中找到 brown ，然后扫描所有列，找到包含 brown 的文档。我们可以快速看到 Doc_1 和 Doc_2 包含 brown 这个 token。
 
 然后，对于聚合部分，我们需要找到 Doc_1 和 Doc_2 里所有唯一的词项。 用倒排索引做这件事情代价很高： 我们会迭代索引里的每个词项并收集 Doc_1 和 Doc_2 列里面 token。这很慢而且难以扩展：随着词项和文档的数量增加，执行时间也会增加。
 
 Doc values 通过转置两者间的关系来解决这个问题。倒排索引将词项映射到包含它们的文档，doc values 将文档映射到它们包含的词项：
 
 Doc      Terms
 -----------------------------------------------------------------
 Doc_1 | brown, dog, fox, jumped, lazy, over, quick, the
 Doc_2 | brown, dogs, foxes, in, lazy, leap, over, quick, summer
 Doc_3 | dog, dogs, fox, jumped, over, quick, the
 -----------------------------------------------------------------
 当数据被转置之后，想要收集到 Doc_1 和 Doc_2 的唯一 token 会非常容易。获得每个文档行，获取所有的词项，然后求两个集合的并集。
 
 因此，搜索和聚合是相互紧密缠绕的。搜索使用倒排索引查找文档，聚合操作收集和聚合 doc values 里的数据。
 
 #深入文档值
 在上一节一开头我们就说文档值（doc values） 是 "更快、更高效并且内存友好" 。 听起来好像是不错的营销术语，不过话说回来文档值到底是如何工作的呢？
 
 文档值是在索引时与倒排索引同时产生的。也就是说文档值是按段来产生的并且是不可变的，正如用于搜索的倒排索引一样。 同样，和倒排索引一样，文档值也序列化到磁盘。这些对于性能和伸缩性很重要。
 
 通过序列化一个持久化的数据结构到磁盘，我们可以依赖于操作系统的缓存来管理内存，而不是在 JVM 堆栈里驻留数据。 当 “工作集（working set）” 数据要小于系统可用内存的情况下，操作系统会自然的将文档值驻留在内存，这将会带来和直接使用 JVM 堆栈数据结构相同的性能。
 
 不过，如果你的工作集远大于可用内存，操作系统会开始根据需要对文档值进行分页开/关。这会显著慢于纯内存驻留的数据结构，当然，它也拥有使用远大于服务器内存容量的伸缩性的好处。 如果这些数据结构是纯粹的存储于 JVM 堆内存，那么唯一的选项只能是随着内存溢出（OutOfMemory）而崩溃（或是实现一个分页模式，正如操作系统的那样）。
 
 注意
 因为文档值不是由 JVM 来管理，所以 Elasticsearch 服务器可以配置一个很小的 JVM 堆栈。 这会给操作系统带来更多的内存来做缓存。同时也带来一个好处就是让 JVM 的垃圾回收器工作在一个很小的堆栈，结果就是更快更高效的回收周期。
 
 传统上，我们会建议分配机器内存的 50% 来给 JVM 堆栈。随着文档值的引入，这个建议开始不再适用。 在 64gb 内存的机器上，也许可以考虑给堆栈分配 4-16gb 的内存，而不是之前建议的 32gb。
 
 有关更详细的讨论，查看 堆内存:大小和交换.
 
 ##列式存储的压缩
 从广义来说，文档值本质上是一个序列化的 列式存储 。 正如我们上一节所讨论的，列式存储 擅长某些操作，因为这些数据的存储天然适合这些查询。
 
 而且，他们也同样擅长数据压缩，特别是数字。 这对于节省磁盘空间和快速访问很重要。现代 CPU 的处理速度要比磁盘快几个数量级（尽管即将到来的 NVMe 驱动器正在迅速缩小差距）。 这意味着减少必须从磁盘读取的数据量总是有益的，尽管需要额外的 CPU 运算来进行解压。
 
 要了解它如何帮助压缩数据，来看一组数字类型的文档值：
 
 Doc      Terms
 -----------------------------------------------------------------
 Doc_1 | 100
 Doc_2 | 1000
 Doc_3 | 1500
 Doc_4 | 1200
 Doc_5 | 300
 Doc_6 | 1900
 Doc_7 | 4200
 -----------------------------------------------------------------
 按列布局意味着我们有一个连续的数据块： [100,1000,1500,1200,300,1900,4200] 。因为我们已经知道他们都是数字（而不是像文档或行中看到的异构集合），所以我们可以使用统一的偏移来将他们紧紧排列。
 
 而且，针对这样的数字有很多种压缩技巧。 你会注意到这里每个数字都是 100 的倍数，文档值会检测一个段里面的所有数值，并使用一个 最大公约数 ，方便做进一步的数据压缩。
 
 如果我们保存 100 作为此段的除数，我们可以对每个数字都除以 100，然后得到： [1,10,15,12,3,19,42] 。现在这些数字变小了，只需要很少的位就可以存储下，也减少了磁盘存放的大小。
 
 文档值正是使用了像这样的一些技巧。它会按依次检测以下压缩模式:
 
 如果所有的数值各不相同（或缺失），设置一个标记并记录这些值
 如果这些值小于 256，将使用一个简单的编码表
 如果这些值大于 256，检测是否存在一个最大公约数
 如果没有存在最大公约数，从最小的数值开始，统一计算偏移量进行编码
 你会发现这些压缩模式不是传统的通用的压缩方式，比如 DEFLATE 或是 LZ4。 因为列式存储的结构是严格且良好定义的，我们可以通过使用专门的模式来达到比通用压缩算法（如 LZ4 ）更高的压缩效果。
 
 注意
 你也许会想 "好吧，貌似对数字很好，不知道字符串怎么样？" 通过借助顺序表（ordinal table），字符类型也是类似进行编码的。字符类型是去重之后存放到顺序表的，通过分配一个 ID，然后这些 ID 和数值类型的文档值一样使用。 也就是说，字符类型和数值类型一样拥有相同的压缩特性。
 
 顺序表本身也有很多压缩技巧，比如固定长度、变长或是前缀字符编码等等。
 
 ##禁用文档值
 文档值默认对所有字段启用，除了分析字符类型字段。也就是说所有的数字、地理坐标、日期、IP 和不分析（ not_analyzed ）字符类型。
 
 分析字符类型暂时还不使用文档值。分析流程会产生很多新的 token，这会让文档值不能高效的工作。我们将在 聚合与分析 讨论如何使用分析字符类型来做聚合。
 
 因为文档值默认启用，你可以选择对你数据集里面的大多数字段进行聚合和排序操作。但是如果你知道你永远也不会对某些字段进行聚合、排序或是使用脚本操作？
 
 尽管罕见，但当这些情况出现时，你还是希望有办法来为特定的字段禁用文档值。这回为你节省磁盘空间（因为文档值再也没有序列化到磁盘），也许还能提升些许索引速度（因为不需要生成文档值）。
 
 要禁用文档值，在字段的映射（mapping）设置 doc_values: false 即可。例如，这里我们创建了一个新的索引，字段 "session_id" 禁用了文档值：
 
 PUT my_index
 {
   "mappings": {
     "my_type": {
       "properties": {
         "session_id": {
           "type":       "string",
           "index":      "not_analyzed",
           "doc_values": false 
         }
       }
     }
   }
 }
 
 
 通过设置 doc_values: false ，这个字段将不能被用于聚合、排序以及脚本操作
 
 反过来也是可以进行配置的：让一个字段可以被聚合，通过禁用倒排索引，使它不能被正常搜索，例如：
 
 PUT my_index
 {
   "mappings": {
     "my_type": {
       "properties": {
         "customer_token": {
           "type":       "string",
           "index":      "not_analyzed",
           "doc_values": true, 
           "index": "no" 
         }
       }
     }
   }
 }
 
 
 文档值被启用来允许聚合
 
 
 
 索引被禁用了，这让该字段不能被查询/搜索
 
 通过设置 doc_values: true 和 index: no ，我们得到一个只能被用于聚合/排序/脚本的字段。无可否认，这是一个非常罕见的需求，但有时很有用。