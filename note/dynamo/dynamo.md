

CAP理论：
Consistency
Availability
Partition tolerance

背景：
    一个系统的可靠性和可扩展性依赖于它的应用状态如何管理。

dynamo需求：
1、永远可写
2、所有节点都在可信的管理域中，（零跳DHT，每个节点维护足够的路由信息，可以从本地直接路由到相应节点）而不是多跳路由
3、使用dynamo的应用程序不需要支持分层的命名空间（如传统文件系统），也不需要支持复杂的关系模型
4、是为延时敏感的应用程序设计，需要至少99.9%的读取和写入必须在300ms内完成，平均读取速度15ms以下，写入速度30ms以下[SLA Service Level Agreements]

设计原则：
    查询模型：
        简单读，通过一个主键写，无须关系模式。

    最终一致性：
        面临的理论问题： 当网络故障时，强一致性与高可用性不能同时保证。
        解决方法：通过乐观复制提高可用性保证最终一致性，即，变化可以在后台传播到副本，同时可以容忍并发和断开。关键在于冲突检测并协调解决。引入两个问题：何时协调，谁来协调？

    永远可写:即数据的写是最高可用的
        传统存储系统选择在写的过程中协调冲突，从而保证读的复杂度相对简单。问题：如果在给定时间内不能达到所有或大多数副本数，写入可能被拒绝。
        在时间问题上，dynamo选择在读时刻解决冲突，以确保写永不被拒绝。在谁的问题上，dyanmo选择由客户端解决冲突，如合并冲突版本，而不是由服务端解决，如（last write wins）。

    去中心化：
        过去集中化控制设计导致系统中断。(对称性：每个dynamo节点与其他节点责任一致。可简化系统的配置和维护。)

    增量的可扩展性：
        即扩展一个节点时，对系统本身的影响小。

    异质性：
        充分利用不同能力的机器。如：负载的分配与服务器能力成正比。

dynamo paper中的描述：有一个简单的键\值接口，具有高可用性同时具有明确定义的一致性滑动窗口（最终一致性【eventually consistent】），最后在规模增长或请求率上升时具有一个简单的水平扩展方案。

    warn：杨传辉的认为dynamo只提供弱一致性。
    为了便于后续的说明,我们修改 Amazon CTO 关于最终一致性的定义。
    Dynamo 通过 NWR策略提供的最终一致性主要是针对 Dynamo 的多个副本而言的,它们之间保持最终一致。不过对于用户,我们假设 N=3, W=2, R=2 的一种情况,用户先调用 W1 写 A 和 B 两个副本后成
    功返回,接着调用 W2 写 B 和 A 两个副本后成功返回,可能出现在副本 A 上 W1 先于 W2 执行,而在副本 B 上 W2 先于 W1 执行,虽然副本 A 和 B 都能够通过执行满足交换律的合并操作,比如基于”last write wins”的策略进行合并使得最终副本 A 和 B 上的数据完全一致,但是
    可能出现一些异常情况,比如副本 A 和 B 所在的机器时钟不一致,合并的结果是 W1 把 W2给覆盖了,W2 的操作结果消失了。这显然与用户的期望是不一致的。
    为了方便后续对系统进行划分,我们把 Amazon Dynamo 这种需要依赖操作合并,可能会丢失数据的模型从最终一致性模型中排除出去。最终一致性模型要求同一份数据同一时刻只能被一台机器修改,也就是说机器宕机时需要停很短时间写服务。Amazon Dynamo 提
    供的一致性模型我们归类到一般的弱一致性模型中。

dynamo属于线上弱一致性系统。线上弱一致性系统要求读请求在1—～30ms内响应。主要用于应用可以解决冲突的场景，如购物车。该类系统可以采用peertopeer（P2P）的方式实现

---------

划分
技术：一致性哈希
优势：增量的可伸缩性


写的高可用性
技术：矢量时钟与读取过程中的协调
优势：版本大小和更新操作速度脱钩。?

暂时性的失败处理
技术：草率仲裁（sloppy quorum）+暗示提交
优势：即使有的副本不可用，仍可提供高可用性和耐用性

永久故障恢复
技术：基于Merkie树的反熵（anti-entropy）即副本同步
优势：在后台同步不同副本

成员和故障检测
技术：基于gossip的检测协议
优势：保持对称性并且避免使用一个集中保存节点活性信息的服务节点


系统接口：
用MD5对Key做hash产生128位的标示符， 2^128的空间大小
get（key） 返回参数：返回一个对象或一个包含冲突版本和对应上下文对象的列表
put（key,context, object） key决定对象放置位置， context包含系统元数据（对调用者可见）并且包含对象的版本信息。存储时上下文与对象一起存储，以便验证上下文的有效性。
没有提到del接口，不知有没有

----------------------
dyanmo设计：
    对于集群来说，其中的节点是对等的。

    对于每个节点具有的功能：
        1请求协调
        2成员及故障检测
        3本地持久化引擎（可替换如：berkeleyDB，MYSQL）

    对于每个具体的key，有N个副本

    对于client，无须知道所有节点，消息可以发给任意一个节点

----------------------
具体技术：

划分算法：
技术：一致性哈希[consistent hashing]
优势：增量的可伸缩性

    Hash 算法的一个衡量指标是单调性（ Monotonicity ），定义如下：

    单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。
    http://blog.csdn.net/sparkliang/article/details/5279393

    基本一致性哈希：
        将一个哈希函数的输出范围首尾相连构成一个环。每个节点分配一个这个空间的一个随机值。每个具体的key通过hash后落到的位置由这点顺时针向后的一个节点管理。

        优点：
            节点进入或者离开仅影响前后的邻居。
        缺点：
            1无视节点的异质性。
            2随机分配节点位置导致数据分布和负荷的不均匀分布。

    引入虚拟节点的一致性哈希：
        将一个哈希函数的输出范围首尾相连构成一个环。 将一个节点分成多个虚拟节点，将这些虚拟节点的映射到环形空间（虚拟节点的hash称为Token），数据先找到它对应的虚拟节点，再找到该虚拟节点所属的节点。

        优点：
              节点负责的虚拟节点数量由处理能力决定，考虑了异质性。
        缺点：
              1由于虚拟节点位置随机大小随机，随机分配节点位置仍会导致数据分布和负荷的不均匀分布。
              2如果有新节点加入，需要扫描所有节点上的所有数据对象，判断全部数据对象是否需要迁移，这种全局的扫描造成很大的开销。


    dynamo采用的一致性哈希：
        固定所有虚拟节点的大小和位置，只改变虚拟节点和节点的对应关系。
        将整个空间平均分成Q个虚拟节点。每个节点分配Q/S个虚拟节点（假设一共有S个节点）。当有节点加入时，从现有节点每个拿出等量的虚拟节点分给新节点；当有节点离开时，将此节点的所有虚拟节点平均分配给余下的节点，保证系统中每个节点始终都有Q/S个虚拟节点。

        优点：
            1如果一个节点不可用，该节点的负载将均匀的分散到剩余节点。
            2当加入一个新的节点，或不可用节点恢复后，新节点接受其他可用节点的负载大致相当
    参见优化确保均匀负载分布
--------
复制算法：
    以防互联网数据中心失效[IDC failure]

    给定一个N作为副本个数，以某个key所在节点的位置开始，顺时针向后N-1个节点均存储该key。
    如若N=3，则B上的key，将会存储在B，C，D节点上。对于这个Key来说，BCD被称为首选列表（preference list）;
    若站在节点上看，则D节点上存储(A,B],(B,C],(C,D]所有的key。

    一个key的首选列表，需要跳过环上的某些位置，确保该列表落在不同的数据中心上。
--------
仲裁Quorum NRW
    保证副本一致性；对于操作来说，一个读或写，由最慢的那个R或W决定。

    N 副本个数
    R 最小读成功的副本个数
    W 最小写成功副本个数

    要求：
    R+W>N 即满足读和写操作到大多数节点

    N决定每个对象的耐用性, R和W影响对象的耐用性和可用性和一致性

    W=1 R=N //写效率最强，读延时最高
    W=N R=1 //读效率最高，写延时最高

    推荐配置公式：
    R=Q W=Q Q=N/2 + 1

    NRW=322 //常用配置

------
写的高可用性：
    dynamo使用矢量时钟[vector clock]。结构是（物理节点，计数器）列表；
    当客户端想put一个key但最新版本不可用，该项目被put到旧版本上，且不同版本的数据将在之后协调。dynamo中每个结点各自记录自己的版本信息
        1一般情况下，新版本包括老版本，即父子关系。可用语法协调解决冲突。（称为因果顺序）
        2当并发更新与失败同时出现时，会产生冲突版本。需要通过客户端做语义协调。合并出一个版本。（称为平行关系）


    DateA[(nodeX,1)]
    DateB[(nodeX,2)]
    if DateA.getVersion <= DateB.getVersion
       DateB is OK,DataA垃圾回收//语法协调
    else
       //由客户端，协调冲突

    eg：

    1）一个写请求，第一次被节点A处理了。节点A会增加一个版本信息(A，1)。我们把这个时候的数据记做D1(A，1)。
    然后另外一个对同样key的请求还是被A处理了于是有D2(A，2)。这个时候，D2是可以覆盖D1的，不会有冲突产生。

    2）现在我们假设D2传播到了所有节点(B和C)，B和C收到的数据不是从客户产生的，而是别人复制给他们的，
    所以他们不产生新的版本信息，所以现在B和C所持有的数据还是D2(A，2)。于是A，B，C上的数据及其版本号都是一样的。

    3）如果我们有一个新的写请求到了B结点上，于是B结点生成数据D3(A,2; B,1)，
    意思是：数据D全局版本号为3，A升了两新，B升了一次。这不就是所谓的代码版本的log么？

    4）如果D3没有传播到C的时候又一个请求被C处理了，于是，以C结点上的数据是D4(A,2; C,1)。

    5）如果这个时候来了一个读请求，我们要记得，我们的W=1 那么R=N=3，所以R会从所有三个节点上读，此时，他会读到三个版本：

            A结点：D2(A,2)
            B结点：D3(A,2;  B,1);
            C结点：D4(A,2;  C,1)

    6）这个时候可以判断出，D2已经是旧版本（已经包含在D3/D4中），可以舍弃。

    7）但是D3和D4是明显的版本冲突。于是，交给调用方自己去做版本冲突处理。就像源代码版本管理一样。


    对同一个对象
    m客户端写,由X服务器处理
    D1[（SX，1）]
    m再写，由X服务器处理
    D2[（SX，2）]
    m三写，由Y服务器处理
    D3[(SX,2)，(Sy,1)]
    n客户端读取D2，后写，由Z服务器处理
    D4[(SX,2),(Sz,1)]


    k客户端读取到对象列表，发现D3和D4版本冲突，客户端进行协调，然后由X服务器处理
    D5[(Sx,3),(sy,1),(sz,1)]

    消除策略？
    1基于应用场景
    2基于时间戳（默认策略）
    3W=1，R=N，即当作高性能读取引擎，不会出现不一致的情况

    问题1：版本越长，增加门限=10，现实中很难遇到
    http://www.cnblogs.com/yanghuahui/p/3767365.html

--------
读写操作：
    客户端发送请求到dynamo集群的任何一个节点，该节点将请求转发给协调者。
    协调者定义：与某个Key有关的一个节点。

    协调者：
    1通过一致性哈希，选择N个节点
    2将请求转发给N个节点
    3等到R个或者W个响应或者超时
    4如果获得足够的响应后，检查副本的版本
    5响应客户端，
--------
暂时性的失败处理
技术：草率仲裁[sloppy quorum]+暗示提交[hinted handoff]
优势：即使有的副本不可用，仍可提供高可用性和耐用性

    应用场景：节点短暂失效

    为了保证每次都能写到W个副本，读到R个副本，我们每次读和写都是发送给N个节点。（草率仲裁）
    如果这N个节点有节点失效，那么往后继续找一个不同的节点，暂时的代替失效的节点。
    比如，N=3，某个数据的preference list是节点A、B、C，这时A节点失效。那么对于该数据的写请求，将发送到节点B、C、D上。D暂时取代了A的角色，那些原本应该写到A上的数据存放在D中的一个特殊的位置，并记下一个 hint,其包含这次写操作的真正目标节点A的信息。当消息服务收到一个 Gossip信号得知有A节点从失败中恢复过来时，它查看该节点该节点有没有需要移交 (handoff)的数据。通过检查它是否是那个 hint提到的节点，如果是，包含 hint的那个节点将向它移交 (handoff) replica.
    这个技术被称为Hinted Handoff。

--------
永久故障恢复
技术：基于Merkie树的反熵[anti-entropy]即副本同步
优势：在后台同步不同副本

    merkleTree是一种hash树，其叶子节点是key的哈希值。父节点为其所有孩子节点的哈希。
    优点：树的每个分支，可以独立检查，而无须下载整棵树。因此可以最小化数据传输量。

    前提，正由于前面一致性哈希对环上空间的按等分划分，与Merkletree配合使用才更快。
    在节点永久性失效时，需要进行副本同步时，比较两个Merkle tree时，只用比较根节点，根节点相同则这两个树相同，如果根节点不同再比较他的子节点，并以log(n)的速度找到哪些叶子节点不相同。
    Dynamo的每个节点对它的每一个虚拟节点数据集求一个Merkle tree。某个数据集发生变化时只用比较相应的虚拟节点的Merkle tree。Merkle tree减小了节点间比较数据时的对磁盘的读和网络的传输。

------
读修复[read repair]
    对每个请求，在节点上有一个对应的状态机。当一个读请求返回响应后，该状态机会等待一小段时间以接受任何悬而未决的响应。
    如果响应是过时的版本，协调者将用最新版本的数据更新这些过时数据。同时用这个可以消除不必要的副本同步。

-----
成员检测
    gossip算法：
        http://blog.csdn.net/chen77716/article/details/6275762
        来源自八卦消息的流传。又被称为反熵（Anti-Entropy），熵是物理学上的一个概念，代表杂乱无章，而反熵就是在杂乱无章中寻求一致
        优点：去中心化、容错而又最终一致性的算法。收敛性不但得到证明还具有指数级的收敛速度。
        缺点：1不能使用在强一致性的场景下。2冗余通信会对网路带宽、CPU资源造成很大的负载，而这些负载又受限于通信频率，该频率又影响着算法收敛的速度

        在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。每个节点可能知道所有其他节点，也可能仅知道几个邻居节点，
        只要这些节可以通过网络连通，最终他们的状态都是一致的，当然这也是疫情传播的特点。

        Gossip节点的通信方式及收敛性

        根据原论文，两个节点（A、B）之间存在三种通信方式:

            push: A节点将数据(key,value,version)及对应的版本号推送给B节点，B节点更新A中比自己新的数据
            pull：A仅将数据key,version推送给B，B将本地比A新的数据（Key,value,version）推送给A，A更新本地
            push/pull：与pull类似，只是多了一步，A再将本地比B新的数据推送给B，B更新本地

        Gossip节点的协调机制

        协调机制是讨论在每次2个节点通信时，如何交换数据能达到最快的一致性，也即消除两个节点的不一致性。
        协调所面临的最大问题是，因为受限于网络负载，不可能每次都把一个节点上的数据发送给另外一个节点，
        也即每个Gossip的消息大小都有上限。在有限的空间上有效率地交换所有的消息是协调要解决的主要问题。

        在讨论之前先声明几个概念：

            令N = {p,q,s,...}为需要gossip通信的server集合，有界大小
            令(p1,p2,...)是宿主在节点p上的数据，其中数据有(key,value,version)构成，q的规则与p类似。

        为了保证一致性，规定数据的value及version只有宿主节点才能修改，
        其他节点只能间接通过Gossip协议来请求数据对应的宿主节点修改。
        1 精确协调（Precise Reconciliation）

        精确协调希望在每次通信周期内都非常准确地消除双方的不一致性，
        具体表现为相互发送对方需要更新的数据，因为每个节点都在并发与多个节点通信，理论上精确协调很难做到。
        精确协调需要给每个数据项独立地维护自己的version，在每次交互是把所有的(key,value,version)发送到目标进行比对，
        从而找出双方不同之处从而更新。但因为Gossip消息存在大小限制，因此每次选择发送哪些数据就成了问题。
        当然可以随机选择一部分数据，也可确定性的选择数据。
        对确定性的选择而言，可以有最老优先（根据版本）和最新优先两种，最老优先会优先更新版本最新的数据，
        而最新更新正好相反，这样会造成老数据始终得不到机会更新，也即饥饿。

        当然，开发这也可根据业务场景构造自己的选择算法，但始终都无法避免消息量过多的问题。
        2 整体协调（Scuttlebutt Reconciliation）

        整体协调与精确协调不同之处是，整体协调不是为每个数据都维护单独的版本号，
        而是为每个节点上的宿主数据维护统一的version。
        比如节点P会为(p1,p2,...)维护一个一致的全局version，相当于把所有的宿主数据看作一个整体，
        当与其他节点进行比较时，只需必须这些宿主数据的最高version，
        如果最高version相同说明这部分数据全部一致，否则再进行精确协调。

        整体协调对数据的选择也有两种方法：

            广度优先：根据整体version大小排序，也称为公平选择
            深度优先：根据包含数据多少的排序，也称为非公平选择。因为后者更有实用价值，所以原论文更鼓励后者


    dynamo采用gossip协议传播成员的变动，并维持成员的最终一致性。
    每个节点每隔一秒随机选择一个对等节点，两节点间协调其成员变动历史和虚拟节点的token表。

    问题：逻辑分裂，即节点A加入到环，节点B也加入到环。但由于gossip保证的是最终一致性。一段时间内A，B互相不知道对方的存在。
    解决方法：有一些种子节点，Dynamo中的每个节点都知道种子节点，每个节点都首先与种子节点进行虚拟节点表的协调。
------
故障检测
    应用场景：
    1防止读写操作尝试联系无法访问的节点。
    2分区转移（transferring partition）
    3暗示提交

    通过监控本地的成员信息即可。
-------
添加节点/删除节点
    对keyrange的转移

    当一个新的节点(例如 X)添加到系统中时,它被分配一些随机散落在环上的 Token。对于每一个分配给节点 X 的 key range,
    当前负责处理落在其 key range 中的 key 的节点数可能有好几个(小于或等于 N)。由于 key range 的分配指向 X,一些现有的
    节点不再需要存储他们的一部分 key,这些节点将这些 key 传给 X,让我们考虑一个简单的引导(bootstrapping)场景,节点 X
    被添加到图 2 所示的环中 A 和 B 之间,当 X 添加到系统,它负责的 key 范围为(F,G],(G,A]和(A,X]。因此,节点 B,C 和 D
    都各自有一部分不再需要储存 key 范围(在 X 加入前,B 负责(F,G], (G,A], (A,B]; C 负责(G,A], (A,B], (B,C]; D 负责(A,B],
    (B,C], (C,D]。而在 X 加入后,B 负责(G,A], (A,X], (X,B]; C 负责(A,X], (X,B], (B,C]; D 负责(X,B], (B,C], (C,D])。因此,节
    点 B,C 和 D,当收到从 X 来的确认信号时将供出(offer)适当的 key。当一个节点从系统中删除,key 的重新分配情况按一个
    相反的过程进行。

------
优化：
    1确保均匀负载分布
    2平衡性能和耐久性
    3客户端驱动协调或者服务端驱动协调
    4平衡后台与前台任务
------
确保均匀负载分布：
        dynamo假定访问分布是不会高度偏移的，那么一个统一的key可以有助于负载均匀分布。即使访问分布偏移，但流行那些key足够多，通过对这些流行的key进行partition医用可以均匀分散到各个节点。

        策略1：每个节点T个随机Token和基于Token分割：
            Token不仅决定分区对应的节点（也就是分区位置）还决定分区的大小。这种策略的缺点是当节点加入时，分区分裂成两部分，其中一部分的数据要从原有节点中迁移到新加入的节点。由于分区分裂所以原有节点要扫描本地数据以确定哪部分数据要迁移到新节点上，这个过程很费时。此外，每个分区对应一棵Merkle树，分区分裂后需要重新计算Merkle树。
            由于Key Range的随意性，无法为整个key空间做快照，不便于归档。

        策略2：每个节点T个随机Token和相同大小分区。
            既然分区会随节点加入删除动态变化，那么我们就把环形空间划分为Q个相同大小的分区。S代表系统中节点个数，Q>>N 且 Q>>S*T ，即分区个数远大于每次读写涉及的节点个数以及虚拟节点个数。
            将hash空间划分成固定的Q个分区，而Token只负责确定分区的位置。如此，当节点加入时，分区不会分裂因而不需要重新计算分区的Merkle树，并且原节点以分区为单位将数据迁移到新节点不需要扫描本地数据确定哪些数据迁入新节点。即数据分区与节点如何划分脱耦

        策略3：每个节点Q/S个Token和相同大小分区。
            维持总的Token数目和分区数目相同。
            每个Token对应一个分区，这样在均衡性上是最好的。假设分区数目为Q，系统中物理节点数目为S, 那么每个物理节点的Token数目为Q/S。当节点加入时，原有节点的Token数目从Q/S减少到Q/(S+1)，所有减少的Token都分配给了新加入节点的Token（注意：一个Token对应一个分区）。

        疑问：负载均衡方面至少应该考虑每个物理节点的磁盘容量以及如何将副本分布在不同的机架。Dynamo论文中提到可以通过设置每个物理节点的Token数目来表示磁盘容量，磁盘容量大的Token数目多，反之，Token数目少。但论文中提到的第三个优化方案中每个物理节点的Token数目是相同的，都为Q/S，也就是说没有采用这种方法来表征磁盘容量。对于副本如何分布到不同机架的问题，论文也只字未提。

------
平衡性能和耐久性

需要更高的性能时，dynamo牺牲持久性提高性能。即每个存储节点在内存中维护对象缓冲区。
每次写都存缓存后返回，写线程定期将数据落地。
但是，节点崩溃有可能造成写数据丢失，为了减少系统耐用性风险，才用W-1个内存写，1个持久化写。

------
客户端驱动协调或者服务端驱动协调

服务端驱动协调，请求到来后由负载均衡器选择需要服务的节点。

客户端驱动协调，定时更新节点成员状态。然后根据Key得到可以直接提供服务的节点列表。可以减少一跳。但效率取决于成员状态表的新鲜度。

------
平衡后台与前台任务

有一个管理监控器监控前台操作对资源的利用情况，根据反馈来限制后台任务可以使用的资源以免侵扰。




















